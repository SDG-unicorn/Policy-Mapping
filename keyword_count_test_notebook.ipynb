{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python standard library imports\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import re\n",
    "#Scientific Python ecosystem imports\n",
    "import pandas as pd\n",
    "#Text mining packages\n",
    "import nltk as nltk\n",
    "from whoosh.lang.porter import stem\n",
    "#MM Import\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from docx2python import docx2python\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import urllib.request\n",
    "\n",
    "from polmap.polmap import preprocess_text, doc2text # replaced the keyword processing block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.a) Read all files in input directory and select allowed filetypes\n",
    "\n",
    "input_dir = pathlib.Path.cwd() / 'pdf_re' / 'Test' #/ 'Eurlex' /'A_Union_of_Equality_EU_Roma_strategic_framework_for_equality,_inclusion_and_participation' / '52020DC0620' #MM let user provide an input dir\n",
    "input_folder_name = input_dir.name\n",
    "\n",
    "allowed_filetypes=['.pdf','.html','.mhtml','.doc','.docx']\n",
    "\n",
    "files = sorted(input_dir.glob('**/*.*'))\n",
    "files = [ file for file in files if file.suffix in allowed_filetypes]\n",
    "print(*files, sep='\\n')\n",
    "#MM assert files==False and log assertion error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.b) Create output folder structure based on input name, date and time of exectution\n",
    "\n",
    "date = dt.datetime.now().date().isoformat() #def make_directories(project='TEI'): #MM start func definition\n",
    "hour = dt.datetime.now().time().isoformat(timespec='seconds').replace(':', '')\n",
    "current_date = '_'+date+'_T'+hour\n",
    "\n",
    "project_title = input_folder_name+'_dev_ipynb_out'#+str(current_date) \n",
    "\n",
    "out_dir = pathlib.Path.cwd() / 'output' / project_title #Beginning of try block\n",
    "log_dir = out_dir / 'logs'\n",
    "results_dir = out_dir / 'results'\n",
    "docs2txt_dir = out_dir / 'docs2txt'\n",
    "stemmed_doctext_dir = out_dir / 'docs2txt_stemmed'\n",
    "\n",
    "dir_dict = { directory: directory.mkdir(mode=0o777, parents=True, exist_ok=True) for directory in [out_dir, log_dir, results_dir, docs2txt_dir, ] } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "########### 2) MM Read the list of keywords and apply the prepare_keyords text processing function from polmap\n",
    "\n",
    "keys = pd.read_excel('keys_update_15012020.xlsx', sheet_name= 'Target_keys' ) #MM 'keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet1' \n",
    "goal_keys = pd.read_excel('keys_update_15012020.xlsx', sheet_name= 'Goal_keys' ) #MM Create a dictionary of dataframes for each sheet\n",
    "dev_count_keys = pd.read_excel('keys_update_15012020.xlsx', sheet_name= 'MOI' ) #MM 'keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet2' \n",
    "\n",
    "#remove all from stop_words to keep in keywords\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stop_words.remove(\"all\")\n",
    "\n",
    "keys['Keys']=keys['Keys'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "goal_keys['Keys']=goal_keys['Keys'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "dev_count_keys['Keys']=dev_count_keys['Keys'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "\n",
    "##Country names\n",
    "countries_in = pd.read_excel('keys_update_15012020.xlsx', sheet_name= 'developing_countries') #MM 'keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'developing_countries'\n",
    "countries = countries_in['Name'].values.tolist()\n",
    "country_ls = []\n",
    "for element in countries:\n",
    "    element = [re.sub(r\"[^a-zA-Z-]+\", '', t.lower().strip()) for t in element.split()]\n",
    "    # countries = [x.strip(' ') for x in countries]\n",
    "    element = [stem(word) for word in element if not word in stop_words]\n",
    "    element = ' '.join(element)\n",
    "    country_ls.append(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doctext_dict = {}\n",
    "PDFtext=[]\n",
    "counter = 0\n",
    "for doc_path in files:\n",
    "    counter += 1\n",
    "    try:\n",
    "        policy_text=[]\n",
    "        doc_text = doc2text(doc_path)\n",
    "        policy_text.append(doc_text)\n",
    "        doctext_ = doc_path.parts[doc_path.parts.index(input_dir.name)+1:]\n",
    "        doctext_name =  docs2txt_dir.joinpath(*doctext_)\n",
    "        doctext_name.parent.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "        doctext_name = doctext_name.parent.joinpath(doctext_name.name.replace('.','_')+'.txt')\n",
    "        with open(doctext_name, 'w') as file_:\n",
    "           file_.write(doc_text)\n",
    "        PDFtext.append(['/'.join(doctext_),' ; '.join(policy_text)])\n",
    "        PDFtext\n",
    "        #doctext_dict['/'.join(doctext_)]=' ; '.join(policy_text)])\n",
    "    except Exception as excptn: #MM I'd log errors as described in https://realpython.com/python-logging/, we need to test this.\n",
    "        logging.exception('{doc_file} raised exception {exception} \\n\\n'.format(doc_file=doc_item.name, exception=excptn))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*PDFtext, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "########### 4) Read document files and convert them into text\n",
    "\n",
    "PDFtext_cpy = copy.deepcopy(PDFtext)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "for item in PDFtext_cpy:\n",
    "    #detect soft hyphen that separates words\n",
    "    item[1] = item[1].replace('   ', '\\t') #Replace triple spaces with tabs (useful for footnotes)\n",
    "    item[1] = item[1].replace('  ', ' ') #Remove double spaces (Frequent in text body)\n",
    "    item[1] = re.sub(r'\\t{2,} \\n', r'', item[1]) #Remove a repetition of tabs ending with a space and a line return\n",
    "    item[1] = re.sub(r'([a-zA-Z0-9,:?!())]+) \\n([a-zA-Z0-9()]+)', r'\\1 \\2', item[1]) #Remove line returns between words and commas\n",
    "    item[1] = re.sub(r'([a-zA-Z0-9)]+)([.,:;?!][^a-zA-Z0-9])', r'\\1 \\2', item[1]) #Add a space between words and punctation\n",
    "    #item[1] = re.sub(r'(\\w+)\\. \\n?', r'\\1.\\n', item[1]) #Add line returns after each point    \n",
    "    item[1] = re.sub(r'-\\n', '-', item[1]) #\n",
    "    item[1] = re.sub(r'(\\d+ \\n\\n\f \\n)', r'Page \\1', item[1]) #Find page breaks and add a leading 'Page' string.\n",
    "    item[1] = re.sub(r' \\n\\n\f \\n', r'', item[1]) #Remove the sequence of FF, space and line returns at the end of each page.\n",
    "    item[1] = re.sub(r'(\\d+\\t[A-Z]\\w?)(.*\\n*.*)( \\n)', r'Ref:\\1\\n', item[1]) # Find footnotes and add a leadin 'Ref:' string.\n",
    "    #Regex for one line footnotes r'\\d+\\t[A-Z]\\w?.*\\n*.* \\n' #((\\d+\\t[A-Z]\\w?)(.[\\s]*)(\\.\\n?))\n",
    "    item[1] = re.sub(r'(Ref:\\d+\\t[A-Z]\\w?)([\\s\\S]*?)(Page \\d+)', r'', item[1]) #Remove anything between footnotes and page breaks included.\n",
    "    item[1] = re.sub(r'\\n \\n', r'', item[1])\n",
    "    item[1] = re.sub(r'\\n{4,}', r'', item[1])\n",
    "    test=item[1]\n",
    "    #item[1] = [ for t in item[1].split()]\n",
    "    # #get indices of soft hyphens\n",
    "    indices = [i for i, s in enumerate(item[1]) if '\\xad' in s]\n",
    "    #merge the separated words\n",
    "    # for index in indices:\n",
    "    #     item[1][index] = item[1][index].replace('\\xad', '')\n",
    "    #     item[1][index+1] = item[1][index]+item[1][index+1]\n",
    "    # #remove unnecessary list elements\n",
    "    # for index in sorted(indices, reverse=True):\n",
    "    #     del item[1][index]\n",
    "    # #remove special character, numbers, lowercase #MM from here until @ this code is identical to prepare keywords correct?\n",
    "    # item[1] = [re.sub(r\"[^a-zA-Z-\\.]+\", '', t.lower().strip()) for t in item[1]]\n",
    "    # #add whitespaces\n",
    "    # item[1] = [word.center(len(word)+2) for word in item[1]]\n",
    "    # #recover R&D for detection\n",
    "    # item[1] = [w.replace(\" rd \", \"R&D\") for w in item[1]]\n",
    "    # # remove words > 2\n",
    "    # item[1] = [word for word in item[1] if len(word) > 2 or word == \"ph\"]\n",
    "    # # remove '\n",
    "    # # item[1] = [s.replace('\\'', '') for s in item[1]]\n",
    "    # #remove whitespaces\n",
    "    # item[1] = [x.strip(' ') for x in item[1]]\n",
    "    # #add special char to prevent aids from being stemmed to aid\n",
    "    # item[1] = [w.replace(\"aids\", \"ai&ds&\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"productivity\", \"pro&ductivity&\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"remittances\", \"remit&tance&\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"remittance\", \"remit&tance&\") for w in item[1]]\n",
    "    # # stem words\n",
    "    # item[1] = [stem(word) for word in item[1] if not word in stop_words]\n",
    "    # #remove special char for detection in text\n",
    "    # item[1] = [w.replace(\"ai&ds&\", \"aids\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"pro&ductivity&\", \"productivity\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"remit&tance&\", \"remittance\") for w in item[1]]\n",
    "    # #try lemmatizing\n",
    "    # # item[1] = [lemmatizer.lemmatize(word) for word in item[1] if not word in stop_words]\n",
    "    # # merge back together to 1 string\n",
    "    #item[1] = ' '.join(item[1])\n",
    "    # #add trailing leading whitespace\n",
    "    # item[1] = \" \" + item[1] + \" \"\n",
    "    # #save out\n",
    "    item_path = stemmed_doctext_dir / pathlib.PurePath(item[0]) #stemmed_doctext_dir / pathlib.PurePath(item[0])\n",
    "    item_path.parent.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "    item_path = item_path.parent.joinpath(item_path.name.replace('.','_')+'_stemmed.txt')\n",
    "    with open(item_path, 'w', encoding='utf-8') as stemdoctext:\n",
    "           stemdoctext.write(item[1]+'\\n\\nTextlenght: {}'.format(len(item[1])))\n",
    "    #Append textlenght\n",
    "    item = item.append(len(item[1])) #MM @\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "3c701ea15ff6b8cd345d7eda6702a5a5acbef34c5d809ba6ada83c604f672a97"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}