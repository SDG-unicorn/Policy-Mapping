{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python standard library imports\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import re\n",
    "#Scientific Python ecosystem imports\n",
    "import pandas as pd\n",
    "#Text mining packages\n",
    "import nltk as nltk\n",
    "from whoosh.lang.porter import stem\n",
    "#MM Import\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from docx2python import docx2python\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import urllib.request\n",
    "\n",
    "from polmap.polmap import preprocess_text, doc2text # replaced the keyword processing block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/Eurlex/The_EU_Green_Deal_52019DC0640/CELEX_52019DC0640_EN_TXT.html\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/Eurlex/The_EU_Green_Deal_52019DC0640/cellar_b828d165-1c22-11ea-8c1f-01aa75ed71a1.0002.01_DOC_1.doc\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/Eurlex/The_EU_Green_Deal_52019DC0640/cellar_b828d165-1c22-11ea-8c1f-01aa75ed71a1.0002.01_DOC_2.doc\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/Eurlex/The_EU_Green_Deal_52019DC0640/cellar_b828d165-1c22-11ea-8c1f-01aa75ed71a1.0002.02_DOC_1.pdf\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/Eurlex/The_EU_Green_Deal_52019DC0640/cellar_b828d165-1c22-11ea-8c1f-01aa75ed71a1.0002.02_DOC_2.pdf\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/JPB/JRC_group_A/WP_2021_P_PP_20000_PRJ_30000_08012021.docx\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/JPB/JRC_group_A/WP_2021_P_PP_20000_PRJ_30025_08012021.docx\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/JPB/JRC_group_B/WP_2021_P_PP_20020_PRJ_30027_08012021.docx\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/JPB/JRC_group_B/WP_2021_P_PP_20020_PRJ_30040_08012021.docx\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/TEI/AFRICA/1.Cameroon TEI Stability_FR_EN.pdf\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/TEI/AFRICA/2.DRC TEI Alliance verte_FR_EN.pdf\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/TEI/ASIA/1. Bangladesh - Decent work.pdf\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/TEI/ASIA/2. Cambodia - Landscapes Forest Agriculture.pdf\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/TEI/LAC/1.Brazil TEI Green Deal.pdf\n/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/TEI/LAC/2.Chile TEI Renewable Energy.pdf\n"
     ]
    }
   ],
   "source": [
    "## 1.a) Read all files in input directory and select allowed filetypes\n",
    "\n",
    "input_dir = pathlib.Path.cwd() / 'pdf_re' / 'Test' #MM let user provide an input dir\n",
    "input_folder_name = input_dir.name\n",
    "\n",
    "allowed_filetypes=['.pdf','.html','.mhtml','.doc','.docx']\n",
    "\n",
    "files = sorted(input_dir.glob('**/*.*'))\n",
    "files = [ file for file in files if file.suffix in allowed_filetypes]\n",
    "print(*files, sep='\\n')\n",
    "#MM assert files==False and log assertion error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## 1.b) Create output folder structure based on input name, date and time of exectution\n",
    "\n",
    "date = dt.datetime.now().date().isoformat() #def make_directories(project='TEI'): #MM start func definition\n",
    "hour = dt.datetime.now().time().isoformat(timespec='seconds').replace(':', '')\n",
    "current_date = '_'+date+'_T'+hour\n",
    "\n",
    "project_title = input_folder_name+str(current_date) \n",
    "\n",
    "out_dir = pathlib.Path.cwd() / 'output' / project_title #Beginning of try block\n",
    "log_dir = out_dir / 'logs'\n",
    "results_dir = out_dir / 'results'\n",
    "docs2txt_dir = out_dir / 'docs2txt'\n",
    "stemmed_doctext_dir = out_dir / 'docs2txt_stemmed'\n",
    "\n",
    "dir_dict = { directory: directory.mkdir(mode=0o777, parents=True, exist_ok=True) for directory in [out_dir, log_dir, results_dir, docs2txt_dir, ] } "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "########### 2) MM Read the list of keywords and apply the prepare_keyords text processing function from polmap\n",
    "\n",
    "keys = pd.read_excel('keys_update_15012020.xlsx', sheet_name= 'Target_keys' ) #MM 'keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet1' \n",
    "goal_keys = pd.read_excel('keys_update_15012020.xlsx', sheet_name= 'Goal_keys' ) #MM Create a dictionary of dataframes for each sheet\n",
    "dev_count_keys = pd.read_excel('keys_update_15012020.xlsx', sheet_name= 'MOI' ) #MM 'keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet2' \n",
    "\n",
    "#remove all from stop_words to keep in keywords\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stop_words.remove(\"all\")\n",
    "\n",
    "keys['Keys']=keys['Keys'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "goal_keys['Keys']=goal_keys['Keys'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "dev_count_keys['Keys']=dev_count_keys['Keys'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "\n",
    "##Country names\n",
    "countries_in = pd.read_excel('keys_update_15012020.xlsx', sheet_name= 'developing_countries') #MM 'keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'developing_countries'\n",
    "countries = countries_in['Name'].values.tolist()\n",
    "country_ls = []\n",
    "for element in countries:\n",
    "    element = [re.sub(r\"[^a-zA-Z-]+\", '', t.lower().strip()) for t in element.split()]\n",
    "    # countries = [x.strip(' ') for x in countries]\n",
    "    element = [stem(word) for word in element if not word in stop_words]\n",
    "    element = ' '.join(element)\n",
    "    country_ls.append(element)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#doctext_dict = {}\n",
    "PDFtext=[]\n",
    "counter = 0\n",
    "for doc_path in files:\n",
    "    counter += 1\n",
    "    try:\n",
    "        policy_text=[]\n",
    "        doc_text = doc2text(doc_path)\n",
    "        while '\\n\\n\\n\\n' in doc_text : doc_text = doc_text.replace('\\n\\n\\n\\n', '\\n\\n\\n') #docx2python specific fix. would probably fit better elsewhere\n",
    "        policy_text.append(doc_text)\n",
    "        doctext_ = doc_path.parts[doc_path.parts.index(input_dir.name)+1:]\n",
    "        doctext_name =  docs2txt_dir.joinpath(*doctext_)\n",
    "        doctext_name.parent.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "        doctext_name = doctext_name.parent.joinpath(doctext_name.stem+'.txt')\n",
    "        with open(doctext_name, 'w') as file_:\n",
    "           file_.write(doc_text)\n",
    "        PDFtext.append(['/'.join(doctext_),' ; '.join(policy_text)])\n",
    "        PDFtext\n",
    "        #doctext_dict['/'.join(doctext_)]=' ; '.join(policy_text)])\n",
    "    except Exception as excptn: #MM I'd log errors as described in https://realpython.com/python-logging/, we need to test this.\n",
    "        logging.exception('{doc_file} raised exception {exception} \\n\\n'.format(doc_file=doc_item.name, exception=excptn))\n",
    "\n",
    "PDFtext_cpy = copy.deepcopy(PDFtext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(*PDFtext_cpy, sep='\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(PDFtext_cpy[5][1]+'\\n\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "########### 4) Read document files and convert them into text\n",
    "\n",
    "PDFtext = copy.deepcopy(PDFtext_cpy)\n",
    "\n",
    "lemmatizer = nltk.stem.WordNetLemmatizer()\n",
    "for item in PDFtext:\n",
    "    #detect soft hyphen that separates words\n",
    "    item[1] = item[1].replace('.', ' .')\n",
    "    item[1] = [re.sub(r'-\\n', '', t) for t in item[1].split()]\n",
    "    # #get indices of soft hyphens\n",
    "    indices = [i for i, s in enumerate(item[1]) if '\\xad' in s]\n",
    "    #merge the separated words\n",
    "    for index in indices:\n",
    "        item[1][index] = item[1][index].replace('\\xad', '')\n",
    "        item[1][index+1] = item[1][index]+item[1][index+1]\n",
    "    print(str(item[1])+'\\n\\n')\n",
    "    # #remove unnecessary list elements\n",
    "    # for index in sorted(indices, reverse=True):\n",
    "    #     del item[1][index]\n",
    "    # #remove special character, numbers, lowercase #MM from here until @ this code is identical to prepare keywords correct?\n",
    "    # item[1] = [re.sub(r\"[^a-zA-Z-\\.]+\", '', t.lower().strip()) for t in item[1]]\n",
    "    # #add whitespaces\n",
    "    # item[1] = [word.center(len(word)+2) for word in item[1]]\n",
    "    # #recover R&D for detection\n",
    "    # item[1] = [w.replace(\" rd \", \"R&D\") for w in item[1]]\n",
    "    # # remove words > 2\n",
    "    # item[1] = [word for word in item[1] if len(word) > 2 or word == \"ph\"]\n",
    "    # # remove '\n",
    "    # # item[1] = [s.replace('\\'', '') for s in item[1]]\n",
    "    # #remove whitespaces\n",
    "    # item[1] = [x.strip(' ') for x in item[1]]\n",
    "    # #add special char to prevent aids from being stemmed to aid\n",
    "    # item[1] = [w.replace(\"aids\", \"ai&ds&\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"productivity\", \"pro&ductivity&\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"remittances\", \"remit&tance&\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"remittance\", \"remit&tance&\") for w in item[1]]\n",
    "    # # stem words\n",
    "    # item[1] = [stem(word) for word in item[1] if not word in stop_words]\n",
    "    # #remove special char for detection in text\n",
    "    # item[1] = [w.replace(\"ai&ds&\", \"aids\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"pro&ductivity&\", \"productivity\") for w in item[1]]\n",
    "    # item[1] = [w.replace(\"remit&tance&\", \"remittance\") for w in item[1]]\n",
    "    # #try lemmatizing\n",
    "    # # item[1] = [lemmatizer.lemmatize(word) for word in item[1] if not word in stop_words]\n",
    "    # # merge back together to 1 string\n",
    "    # item[1] = ' '.join(item[1])\n",
    "    # #add trailing leading whitespace\n",
    "    # item[1] = \" \" + item[1] + \" \"\n",
    "    # #save out\n",
    "    # # item_path = stemmed_doctext_dir / pathlib.PurePath(item[0]) #stemmed_doctext_dir / pathlib.PurePath(item[0])\n",
    "    # # item_path.parent.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "    # # item_path = item_path.parent.joinpath(item_path.stem+'_stemmed.txt')\n",
    "    # # with open(item_path, 'w') as stemdoctext:\n",
    "    # #        stemdoctext.write(item[1]+'\\n\\nTextlenght: {}'.format(len(item[1])))\n",
    "    # # #Append textlenght\n",
    "    # # item = item.append(len(item[1])) #MM @"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "/mnt/d/OneDrive/SDG/Policy-Mapping/pdf_re/Test/Eurlex/The_EU_Green_Deal_52019DC0640/CELEX_52019DC0640_EN_TXT.html\n<class 'str'>\n"
     ]
    }
   ],
   "source": [
    "print(files[0])\n",
    "\n",
    "with open(files[0], 'r') as file_:\n",
    "\n",
    "    #html_object = urllib.request.urlopen(file_).read()\n",
    "    bs_text = BeautifulSoup(file_).get_text()\n",
    "\n",
    "    file_read_ = file_.read()\n",
    "\n",
    "    html_to_text = html2text.html2text(file_.read()) #BeautifulSoup(files[0])\n",
    "\n",
    "    print(type(html_to_text))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('bs4_text.txt', 'w') as bs4_file:\n",
    "    bs4_file.write(bs_text)\n",
    "\n",
    "with open('html2text_text.txt', 'w') as h2t_file:\n",
    "    h2t_file.write(html_to_text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "BadZipFile",
     "evalue": "File is not a zip file",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mBadZipFile\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-dc4e7a516b15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mdocx2python\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfiles\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtext\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/miniconda3/envs/polmap/lib/python3.9/site-packages/docx2python/main.py\u001b[0m in \u001b[0;36mdocx2python\u001b[0;34m(docx_filename, image_folder, html, extract_image)\u001b[0m\n\u001b[1;32m     32\u001b[0m     \u001b[0;34m:\u001b[0m\u001b[0;32mreturn\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mDocxContent\u001b[0m \u001b[0mobject\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     33\u001b[0m     \"\"\"\n\u001b[0;32m---> 34\u001b[0;31m     \u001b[0mzipf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mzipfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdocx_filename\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     35\u001b[0m     \u001b[0mcontext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_context\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mzipf\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     36\u001b[0m     \u001b[0mcontext\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"do_html\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhtml\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/polmap/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file, mode, compression, allowZip64, compresslevel, strict_timestamps)\u001b[0m\n\u001b[1;32m   1255\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1256\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'r'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1257\u001b[0;31m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_RealGetContents\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1258\u001b[0m             \u001b[0;32melif\u001b[0m \u001b[0mmode\u001b[0m \u001b[0;32min\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m'w'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'x'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1259\u001b[0m                 \u001b[0;31m# set the modified flag so central directory gets written\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/polmap/lib/python3.9/zipfile.py\u001b[0m in \u001b[0;36m_RealGetContents\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   1322\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1323\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mendrec\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1324\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mBadZipFile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"File is not a zip file\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdebug\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1326\u001b[0m             \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mendrec\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mBadZipFile\u001b[0m: File is not a zip file"
     ]
    }
   ],
   "source": [
    "docx2python(files[0]).text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('polmap': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3c701ea15ff6b8cd345d7eda6702a5a5acbef34c5d809ba6ada83c604f672a97"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}