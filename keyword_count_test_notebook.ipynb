{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python standard library imports\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import re\n",
    "#Scientific Python ecosystem imports\n",
    "import pandas as pd\n",
    "#Text mining packages\n",
    "import nltk as nltk\n",
    "from whoosh.lang.porter import stem\n",
    "\n",
    "#The local library where we will move the functions\n",
    "#we design and condense from the original keyword_count.py\n",
    "from polmap.polmap import prepare_keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Options for pandas visualization\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_seq_item', None)\n",
    "pd.set_option('precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 2) read in all keywords #MM this could be moved below after the pdf conversion and before the counting, to have a little bit more of a flow:\n",
    "#e.g. 1) create global variable, 2) convert doc to text 3) load keywords, lemmatize keywords and text, 4) count keys in text 5) save output\n",
    "\n",
    "#MM_ set global viariables for text mining.\n",
    "\n",
    "##process words - remove stopwords, stemming, lemmatizing\n",
    "\n",
    "#set stop words\n",
    "#stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "#remove all from stop_words to keep in keywords\n",
    "#stop_words.remove(\"all\")\n",
    "\n",
    "#MM_process and edit keys\n",
    "\n",
    "keys = pd.read_excel('keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet1' )\n",
    "keys2 = pd.read_excel('keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet1' )\n",
    "#read in 1 row with keys per target\n",
    "# split row by ; - series of list of strings\n",
    "\n",
    "keys['Keys'] = keys['Keys'].str.split(pat = \";\")\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "for i in range(0, len(keys['Keys'])): #applymap instead of 3 nested for loops?\n",
    "    #print(keys['Keys'][i])\n",
    "    for j in range(0,len(keys['Keys'][i])):\n",
    "                # keys['Keys'][i][j] = word_tokenize(keys['Keys'][i][j])\n",
    "        keys['Keys'][i][j] = [re.sub(r\"[^a-zA-Z-]+\", '', t.lower().strip()) for t in keys['Keys'][i][j].split()]\n",
    "        #add whitespaces to words\n",
    "        keys['Keys'][i][j] = [word.center(len(word)+2) for word in keys['Keys'][i][j]]\n",
    "        #transform rd back to R&D for later detection\n",
    "        keys['Keys'][i][j] = [w.replace(\" rd \", \"R&D\") for w in keys['Keys'][i][j]]\n",
    "        #remove words > 2\n",
    "        keys['Keys'][i][j] = [word for word in keys['Keys'][i][j] if len(word) > 2 or word == \"ph\"]\n",
    "        # remove '\n",
    "        # keys['Keys'][i][j] = [s.replace('\\'', '') for s in keys['Keys'][i][j]]\n",
    "        # remove whitespaces\n",
    "        keys['Keys'][i][j] = [x.strip(' ') for x in keys['Keys'][i][j]]\n",
    "        # add special char to prevent aids from being stemmed to aid\n",
    "        keys['Keys'][i][j] = [w.replace(\"aids\", \"ai&ds&\") for w in keys['Keys'][i][j]]\n",
    "        keys['Keys'][i][j] = [w.replace(\"productivity\", \"pro&ductivity&\") for w in keys['Keys'][i][j]]\n",
    "        keys['Keys'][i][j] = [w.replace(\"remittances\", \"remit&tance&\") for w in keys['Keys'][i][j]]\n",
    "        #stem words\n",
    "        keys['Keys'][i][j] = [stem(word) for word in keys['Keys'][i][j] if not word in stop_words if word != \"aids\"]\n",
    "        # remove special char for detection in text\n",
    "        keys['Keys'][i][j] = [w.replace(\"ai&ds&\", \"aids\") for w in keys['Keys'][i][j]]\n",
    "        keys['Keys'][i][j] = [w.replace(\"pro&ductivity&\", \"productivity\") for w in keys['Keys'][i][j]]\n",
    "        keys['Keys'][i][j] = [w.replace(\"remit&tance&\", \"remittance\") for w in keys['Keys'][i][j]]\n",
    "        # lemmatizing words\n",
    "        # keys['Keys'][i][j] = [lemmatizer.lemmatize(word) for word in keys['Keys'][i][j] if not word in stop_words]\n",
    "        #merge back together to 1 string\n",
    "        keys['Keys'][i][j] = ' '.join(keys['Keys'][i][j])\n",
    "        #add leading and trailing whitespace\n",
    "        keys['Keys'][i][j] = \" \" + keys['Keys'][i][j] + \" \"\n",
    "        print(keys['Keys'][i][j])\n",
    "\n",
    "\n",
    "#keys.head(170)\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "#remove all from stop_words to keep in keywords\n",
    "stop_words.remove(\"all\")\n",
    "keys2['Keys']=keys2['Keys'].apply(lambda x: prepare_keywords(x, stop_words))\n",
    "keys2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}