{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python standard library imports\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import re\n",
    "#Scientific Python ecosystem imports\n",
    "import pandas as pd\n",
    "#Text mining packages\n",
    "import nltk as nltk\n",
    "from whoosh.lang.porter import stem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Set Options for pandas visualization\n",
    "pd.set_option('max_columns', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_rows', None)\n",
    "pd.set_option('max_colwidth', None)\n",
    "pd.set_option('max_seq_item', None)\n",
    "pd.set_option('precision', 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### 2) read in all keywords #MM this could be moved below after the pdf conversion and before the counting, to have a little bit more of a flow:\n",
    "#e.g. 1) create global variable, 2) convert doc to text 3) load keywords, lemmatize keywords and text, 4) count keys in text 5) save output\n",
    "\n",
    "#MM_ set global viariables for text mining.\n",
    "\n",
    "##process words - remove stopwords, stemming, lemmatizing\n",
    "\n",
    "#set stop words\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "#remove all from stop_words to keep in keywords\n",
    "stop_words.remove(\"all\")\n",
    "\n",
    "#MM_process and edit keys\n",
    "\n",
    "keys = pd.read_excel('keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet1' )\n",
    "keys2 = pd.read_excel('keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet1' )\n",
    "#read in 1 row with keys per target\n",
    "# split row by ; - series of list of strings\n",
    "\n",
    "keys['Keys'] = keys['Keys'].str.split(pat = \";\")\n",
    "\n",
    "#lemmatizer = WordNetLemmatizer()\n",
    "for i in range(0, len(keys['Keys'])): #applymap instead of 3 nested for loops?\n",
    "    #print(keys['Keys'][i])\n",
    "    for j in range(0,len(keys['Keys'][i])):\n",
    "                # keys['Keys'][i][j] = word_tokenize(keys['Keys'][i][j])\n",
    "        keys['Keys'][i][j] = [re.sub(r\"[^a-zA-Z-]+\", '', t.lower().strip()) for t in keys['Keys'][i][j].split()]\n",
    "        #add whitespaces to words\n",
    "        keys['Keys'][i][j] = [word.center(len(word)+2) for word in keys['Keys'][i][j]]\n",
    "        #transform rd back to R&D for later detection\n",
    "        keys['Keys'][i][j] = [w.replace(\" rd \", \"R&D\") for w in keys['Keys'][i][j]]\n",
    "        #remove words > 2\n",
    "        keys['Keys'][i][j] = [word for word in keys['Keys'][i][j] if len(word) > 2 or word == \"ph\"]\n",
    "        # remove '\n",
    "        # keys['Keys'][i][j] = [s.replace('\\'', '') for s in keys['Keys'][i][j]]\n",
    "        # remove whitespaces\n",
    "        keys['Keys'][i][j] = [x.strip(' ') for x in keys['Keys'][i][j]]\n",
    "        # add special char to prevent aids from being stemmed to aid\n",
    "        keys['Keys'][i][j] = [w.replace(\"aids\", \"ai&ds&\") for w in keys['Keys'][i][j]]\n",
    "        keys['Keys'][i][j] = [w.replace(\"productivity\", \"pro&ductivity&\") for w in keys['Keys'][i][j]]\n",
    "        keys['Keys'][i][j] = [w.replace(\"remittances\", \"remit&tance&\") for w in keys['Keys'][i][j]]\n",
    "        #stem words\n",
    "        keys['Keys'][i][j] = [stem(word) for word in keys['Keys'][i][j] if not word in stop_words if word != \"aids\"]\n",
    "        # remove special char for detection in text\n",
    "        keys['Keys'][i][j] = [w.replace(\"ai&ds&\", \"aids\") for w in keys['Keys'][i][j]]\n",
    "        keys['Keys'][i][j] = [w.replace(\"pro&ductivity&\", \"productivity\") for w in keys['Keys'][i][j]]\n",
    "        keys['Keys'][i][j] = [w.replace(\"remit&tance&\", \"remittance\") for w in keys['Keys'][i][j]]\n",
    "        # lemmatizing words\n",
    "        # keys['Keys'][i][j] = [lemmatizer.lemmatize(word) for word in keys['Keys'][i][j] if not word in stop_words]\n",
    "        #merge back together to 1 string\n",
    "        keys['Keys'][i][j] = ' '.join(keys['Keys'][i][j])\n",
    "        #add leading and trailing whitespace\n",
    "        keys['Keys'][i][j] = \" \" + keys['Keys'][i][j] + \" \"\n",
    "        print(keys['Keys'][i][j])\n",
    "\n",
    "\n",
    "#keys.head(170)\n",
    "######################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_keywords(keywrds_string, exception_dict=None):\n",
    "    \"\"\"\n",
    "    Prepare keywords for mapping.\n",
    "    \"\"\"\n",
    "    \n",
    "    if exception_dict==None:\n",
    "        exception_dict = {\"aids\": \"ai&ds&\",\n",
    "                          \"productivity\": \"pro&ductivity&\",\n",
    "                          \"remittances\" : \"remit&tance&\"                 \n",
    "                          }\n",
    "    \n",
    "    reverse_exception_dict={value : key for key, value in exception_dict.items()}\n",
    "    \n",
    "    \n",
    "#     [list(map(lambda term: term.lower().strip(), terms))\n",
    "#                       for terms in keywrds_string]\n",
    "#     keywrds_string = [ [term for term in terms]\n",
    "#                       for terms in keywrds_string ]\n",
    "    \n",
    "    keywrds_string = keywrds_string.split(\";\")\n",
    "    keywrds_list = map(lambda term: term.split(),  keywrds_string)\n",
    "    \n",
    "    keywrds_list = [ [term.lower().strip() for term in terms]\n",
    "                      for terms in keywrds_list ]\n",
    "    keywrds_list = [ [re.sub(r\"[^a-zA-Z-]+\", '', term) for term in terms]\n",
    "                      for terms in keywrds_list ]\n",
    "    keywrds_list = [ [term.center(len(term)+2) for term in terms]\n",
    "                      for terms in keywrds_list ]\n",
    "    keywrds_list = [ [term.replace(\" rd \", \"R&D\") for term in terms]\n",
    "                      for terms in keywrds_list ]\n",
    "    keywrds_list = [ [term for term in terms if len(term) > 2 or term == \"ph\" ]\n",
    "                      for terms in keywrds_list ] \n",
    "    # not sure this is working the way intended, \n",
    "    # if the plan was to drop two characters words,\n",
    "    # it is not  as we are however counting also spaces.\n",
    "    # an easy fix would be to move it before the centering of the terms\n",
    "    keywrds_list = [ [term.strip(' ') for term in terms]\n",
    "                      for terms in keywrds_list ]\n",
    "    \n",
    "    keywrds_list = [ [exception_dict[term] if term in exception_dict.keys() \n",
    "                      else term\n",
    "                      for term in terms]\n",
    "                      for terms in keywrds_list ]\n",
    "    \n",
    "    keywrds_list = [ [ stem(term) for term in terms \n",
    "                      if not term in stop_words if term != \"aids\" ]\n",
    "                      for terms in keywrds_list ]\n",
    "    \n",
    "    keywrds_list = [ [reverse_exception_dict[term] if term in reverse_exception_dict.keys() \n",
    "                      else term\n",
    "                      for term in terms]\n",
    "                      for terms in keywrds_list ]  \n",
    "        \n",
    "    keywrds_list = [ ' '.join(terms) for terms in keywrds_list ]\n",
    "    keywrds_list = [ ' '+terms+' ' for terms in keywrds_list ]\n",
    "    keywrds_list = [terms for terms in keywrds_list if terms!='  ']\n",
    "    \n",
    "    return keywrds_list\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keys2['Keys']=keys2['Keys'].apply(lambda x: prepare_keywords(x))\n",
    "keys2.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
