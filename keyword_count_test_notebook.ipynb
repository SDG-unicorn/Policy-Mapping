{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python standard library imports\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import re\n",
    "#Scientific Python ecosystem imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Text mining packages\n",
    "import nltk as nltk\n",
    "from whoosh.lang.porter import stem\n",
    "#MM Import\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import logging\n",
    "import copy\n",
    "\n",
    "from docx2python import docx2python\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import urllib.request\n",
    "\n",
    "from polmap.polmap import preprocess_text, doc2text # replaced the keyword processing block"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_keywords(a_string, stop_words, exception_dict=None):\n",
    "    \"\"\"\n",
    "    Prepare text for mapping.\n",
    "    \"\"\"\n",
    "    text_string = a_string\n",
    "\n",
    "    if text_string==None: #this should be moved to the prepare keywords wrapper function\n",
    "        return None\n",
    "    # if text_string is not str:\n",
    "    #     raise TypeError('text_string is not a string') \n",
    "    #     #How to return the name of the variable passed by user with format?\n",
    "    # Get error when using it with apply and lambda in pandas\n",
    "    \n",
    "    if exception_dict==None:\n",
    "        exception_dict = {\"aids\": \"ai&ds&\",\n",
    "                          \"productivity\": \"pro&ductivity&\",\n",
    "                          \"remittances\" : \"remit&tance&\"                 \n",
    "                          }\n",
    "    elif exception_dict is not dict:\n",
    "        raise TypeError('exception_dict is not a dict')\n",
    "    \n",
    "    reverse_exception_dict = {value : key for key, value in exception_dict.items()}\n",
    "\n",
    "        \n",
    "    #remove all from stop_words to keep in keywords.\n",
    "    # Review scoping rules in python, this fails with:\n",
    "    # NameError: name 'stop_words' is not defined when called in lambda function\n",
    "    # I would expect the variable to always exist whenever calling the function, but it does not.\n",
    "    #if stop_words==None:\n",
    "    #    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    #    stop_words.remove(\"all\")\n",
    "   \n",
    "    text_string = text_string\n",
    "\n",
    "    text_string = text_string.lower().strip()\n",
    "\n",
    "    text_string = re.sub(r'[^a-zA-Z- ]+', '', text_string)\n",
    "\n",
    "    text_string = re.sub(r'([a-zA-Z-]+)', r' \\1 ', text_string) #Equivalent to center, adds leading and trailing space to the captured group\n",
    "\n",
    "    text_string = text_string.replace(' rd ', 'R&D')\n",
    "\n",
    "    text_string = re.sub(r'([a-zA-Z-]{3,}|ph)', r'\\1', text_string)\n",
    "\n",
    "    # not sure this is working the way intended, \n",
    "    # if the plan was to drop two characters words,\n",
    "    # it is not  as we are however counting also spaces.\n",
    "    # an easy fix would be to move it before the centering of the terms\n",
    "    text_string = text_string.strip(' ')\n",
    "    \n",
    "    for key, value in exception_dict.items(): #Protect exceptions from stemming\n",
    "        text_string = text_string.replace(' '+key+' ', ' '+value+' ')\n",
    "    \n",
    "    text_string = re.sub(r'[a-zA-Z-]+', lambda rgx_word: stem(rgx_word.group()), text_string)\n",
    "\n",
    "    for word in stop_words: #Remove stopwords\n",
    "        text_string = text_string.replace(' '+word+' ', '')\n",
    "\n",
    "    for key, value in exception_dict.items(): #Restore words from exception protection\n",
    "        text_string = text_string.replace(' '+value+' ', ' '+key+' ')\n",
    "\n",
    "    text_string = re.sub(r' {2,}', r' ', text_string)\n",
    "\n",
    "    text_string = ' '+text_string+' '\n",
    "    \n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "########### 2) MM Read the list of keywords and apply the prepare_keyords text processing function from polmap\n",
    "\n",
    "keys = pd.read_excel('keys_update_15012020.xlsx', sheet_name = 'Target_keys' ) #MM 'keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet1'\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stop_words.remove(\"all\")\n",
    "\n",
    "raw_keywords_df=keys['Keys'].str.split(';', expand=True)\n",
    "\n",
    "keys['Keys']=keys['Keys'].apply(lambda x: preprocess_text(x, stop_words))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{\"mightn't\", 'your', 'his', 'myself', 'as', 'than', 'up', 'most', \"hasn't\", 'ours', 'over', \"mustn't\", 'mustn', 'themselves', 'other', \"it's\", 'just', 'which', 'very', 'hasn', 'yourselves', 'when', 'here', 'was', 'too', 'you', 'where', \"shan't\", 'ma', 'below', 'through', 'she', 'not', 'an', 'itself', 'because', 'won', 'a', 'being', 'further', 'who', 'them', 'aren', 'm', 'or', 'the', 'each', \"should've\", 'has', 're', 'about', 'theirs', 'whom', 'by', 'he', 'needn', 'such', 'is', 'out', 'few', 'their', \"shouldn't\", 'if', 'd', 'do', 'can', 'until', 'from', \"you'd\", 'am', \"needn't\", \"you'll\", 'both', 'we', 'had', 'these', 'of', 'ain', 'so', 'herself', 'between', 'me', 'on', 'some', 'with', 'off', 'now', 'any', 'have', 'her', 'only', 's', 'but', \"wouldn't\", 'same', 'will', \"weren't\", 'didn', 'those', \"haven't\", 'don', 'into', 'above', \"doesn't\", 'him', 'own', 'couldn', 'how', 'there', 'they', 'been', 'are', 't', 'hadn', 'nor', 'no', 'yours', 'under', 'at', 'shan', \"didn't\", 'shouldn', \"she's\", 'its', 'what', 'i', 'in', 'o', 'once', \"couldn't\", 'wasn', 'why', 'while', 'should', 've', 'that', \"aren't\", 'this', 'himself', \"that'll\", 'ourselves', \"isn't\", 'after', 'doesn', 'did', \"you're\", 'having', 'does', 'during', 'wouldn', 'more', 'my', 'down', 'haven', 'isn', 'hers', 'be', 'mightn', \"don't\", 'again', 'were', \"you've\", 'before', 'to', 'our', 'against', 'll', \"won't\", 'doing', 'then', 'yourself', 'y', 'weren', \"hadn't\", 'and', 'for', \"wasn't\", 'it'}\n"
     ]
    }
   ],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = keys['Keys'].apply(pd.Series)\n",
    "\n",
    "keywords.replace(to_replace={'':None, np.NaN:None}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_keywords_df.replace(to_replace={'':None, np.NaN:None}, inplace=True)\n",
    "raw_keywords_df.drop(labels=57, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_keywords_df=raw_keywords_df.applymap(lambda x: prepare_keywords(x, stop_words))\n",
    "processed_keywords_df.replace(to_replace={'':None, np.NaN:None}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'comparison' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-23f0bdefa011>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mraw_vs_standard\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mraw_keywords_df\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompare\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeywords\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkeep_shape\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mTrue\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m#comparison[1][1]='=IF(AND(ISBLANK(New_preprocessing!B2),ISBLANK(Standard_processing!B2)),\"\",IF(New_preprocessing!C2=Standard_preprocessing!C2,TRUE,FALSE))'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mcomparison\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'comparison' is not defined"
     ]
    }
   ],
   "source": [
    "new_vs_standard=processed_keywords_df.compare(keywords, keep_shape=True)\n",
    "raw_vs_new = raw_keywords_df.compare(processed_keywords_df, keep_shape=True)\n",
    "raw_vs_standard = raw_keywords_df.compare(keywords, keep_shape=True)\n",
    "#comparison[1][1]='=IF(AND(ISBLANK(New_preprocessing!B2),ISBLANK(Standard_processing!B2)),\"\",IF(New_preprocessing!C2=Standard_preprocessing!C2,TRUE,FALSE))'\n",
    "comparison.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_keywords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords_excel=pd.ExcelWriter('updated_processed_keywords_test.xlsx', engine='xlsxwriter')\n",
    "raw_keywords_df.to_excel(keywords_excel, sheet_name='Raw_keywords')\n",
    "processed_keywords_df.to_excel(keywords_excel, sheet_name='New_preprocessing')\n",
    "keywords.to_excel(keywords_excel, sheet_name='Standard_preprocessing')\n",
    "new_vs_standard.to_excel(keywords_excel, sheet_name='New_vs_Standard')\n",
    "raw_vs_new.to_excel(keywords_excel, sheet_name='Raw_vs_New')\n",
    "raw_vs_standard.to_excel(keywords_excel, sheet_name='Raw_vs_Standard')\n",
    "keywords_excel.save()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exception_dict = {\"aids\": \"ai&ds&\",\n",
    "                    \"productivity\": \"pro&ductivity&\",\n",
    "                    \"remittances\" : \"remit&tance&\"                 \n",
    "                    }\n",
    "\n",
    "for key in exception_dict.keys():\n",
    "    print(key) \n",
    "for value in exception_dict.values():\n",
    "    print(value)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('polmap': conda)",
   "metadata": {
    "interpreter": {
     "hash": "3c701ea15ff6b8cd345d7eda6702a5a5acbef34c5d809ba6ada83c604f672a97"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}