{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re, json, pathlib, logging, time, argparse, pprint \n",
    "import datetime as dt\n",
    "import importlib.resources as rsrc\n",
    "from itertools import chain\n",
    "from operator import itemgetter \n",
    "#\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from whoosh.lang.porter import stem\n",
    "#from googletrans import Translator\n",
    "from pygoogletranslation import Translator\n",
    "import circlify as crcf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import polmap.polmap as plmp\n",
    "import polmap.clean_docs as cldc\n",
    "import postprocess.postprocess as pspr\n",
    "\n",
    "\n",
    "##MM imports\n",
    "#from polmap.polmap import make_directories, preprocess_text, doc2text, SDGrefs_mapper # replaced the keyword processing block\n",
    "\n",
    "#import fromRtoPython\n",
    "\n",
    "from docx2python import docx2python\n",
    " \n",
    "#from polmap.polmap import make_directories, preprocess_text, doc2text # replaced the keyword processing block"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "#stopwordset(stopwords.words('english'))\n",
    "stop_words = set(stopwords.words('english'))-set(['no','not','nor'])\n",
    "#stop_words.remove('all')\n",
    "\n",
    "#print(stop_words)\n",
    "\n",
    "test_keys=test_keys=['innovative infrastructure', 'innovative industry', 'indsutrial innovation', 'industry technology', 'research and development', 'R&D', 'innovative technology', 'green technology', 'Expenditure in Research', 'Expenditure in Innovation', 'personnel employed in RD&I', 'foster innovation', 'Investing in digital infrastructure', 'research and innovation', 'innovation and research', 'Horizon Europe', 'Eurostars', 'EUREKA', 'establish research cells', 'foster research and partnerships', 'set up research partnerships', 'networks for research', 'facilitate innovation', 'foster technological progress', 'foster research cooperation', 'facilitate research cooperation', 'improve research cooperation', 'strengthen research cooperation']\n",
    "\n",
    "for key in test_keys:\n",
    "    print(plmp.preprocess_text(key, stop_words, verbose=False))"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      " innov infrastructur \n",
      " innov . \n",
      " industri \n",
      " indsutri \n",
      " innov \n",
      " industri technolog \n",
      " research develop \n",
      " r&d \n",
      " innov technolog \n",
      " green technolog \n",
      " expenditur research \n",
      " expenditur innov \n",
      " personnel emploi rd&i \n",
      " foster innov \n",
      " invest digit infrastructur \n",
      " research innov \n",
      " innov research \n",
      " horizon europ \n",
      " eurostar \n",
      " eureka \n",
      " establish research cell \n",
      " foster research partnership \n",
      " set research partnership \n",
      " network research \n",
      " facilit innov \n",
      " foster technolog progress \n",
      " foster research cooper \n",
      " facilit research cooper \n",
      " improv research cooper \n",
      " strengthen research cooper \n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "a='foo'\n",
    "print(f'{a}'+'\\nbar')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "input_dir=pathlib.Path('output/DirD_/output/2-doc_text')\n",
    "files = sorted(input_dir.glob('*.txt'))\n",
    "file_df=pd.DataFrame(files,columns=['Path'])\n",
    "file_df['File']=file_df['Path'].apply(lambda x: x.name)\n",
    "file_df['PRJ']=file_df['File'].str.extract(r'_PRJ_(\\d{5})')\n",
    "file_df['PP']=file_df['File'].str.extract(r'_PP_(\\d{5})')\n",
    "\n",
    "file_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc_text={}\n",
    "for file_ in files:\n",
    "    with open(file_, 'r') as f:    \n",
    "        doc_text[file_.name]=f.read()\n",
    "    \n",
    "    cleaned_text, cleaned_dict=cldc.clean_jpb(doc_text[file_.name])#[0]\n",
    "\n",
    "    if all(cleaned_dict.values()):\n",
    "        #print(f'{file_.name}: Ok\\n\\n')\n",
    "        pass\n",
    "    else:\n",
    "        print('\\n\\n',file_.name,'\\n\\n')\n",
    "        for key, value in cleaned_dict.items():        \n",
    "            if not value:\n",
    "                print(f'{key}:  {bool(value)}\\n\\n\\n')\n",
    "          \n",
    "    \n",
    "    destpath=file_.parent/'cleaned'/f'{file_.name}_cleaned.txt'.replace('docx.txt_','docx_')\n",
    "    #print(destpath)\n",
    "    destpath.parent.mkdir(mode=0o777, parents=True, exist_ok=True)\n",
    "    with destpath.open('w') as f_:\n",
    "         f_.write(cleaned_text)\n",
    "\n",
    "    #print(destpath,'\\n'*2, destpath.exists())\n",
    "\n",
    "    destpath= destpath.parent/ destpath.name.replace('txt', f'_{all(cleaned_dict.values())}_.json')\n",
    "    with open(destpath, \"w\") as fjson:\n",
    "        json.dump(cleaned_dict, fjson, indent = 6)\n",
    "    #doc_text[file_.name]=re.findall(r\"SDG\\n\\n([a-zA-z, -]{1,})\\n\", doc_text[file_.name], flags=re.IGNORECASE)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "res_path = pathlib.Path('./output/DirD_cleaned_/output/6-results/results_cleaned.xlsx')\n",
    "\n",
    "filename=res_path.parent / 'DirD_cleaned_'\n",
    "\n",
    "mock_target_df = pd.read_excel(res_path,sheet_name='target_dat') #pd.DataFrame({\n",
    "#     'Name': target_list,\n",
    "#     'Value': [np.random.randint(0, 100) for i in target_list]\n",
    "# })\n",
    "mock_target_df=mock_target_df[['Target','Count']]\n",
    "\n",
    "mock_target_df.rename(columns={\"Target\": \"Name\", \"Count\": \"Value\"}, inplace=True)\n",
    "mock_target_df=mock_target_df.groupby(by=['Name']).sum().\n",
    "mock_target_df =  mock_target_df[mock_target_df['Value'] != 0]\n",
    "\n",
    "mock_goals = pd.read_excel(res_path,sheet_name='goal_overview')\n",
    "mock_goals = mock_goals['Count']"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mock_target_df.reset_index().head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mock_target_df['Value'].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "mock_target_df['Name'].tolist()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "doc_text[0]"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "#for name, value in doc_text.items():\n",
    "    #print(name, value)\n",
    "\n",
    "file_df['SDG']=file_df['File'].apply(lambda x: doc_text[x][0])\n",
    "file_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dird_df=pd.read_excel('input/Projects_overview.xlsx')\n",
    "dird_df.head()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "file_df['PRJ']=file_df['PRJ'].astype(str)\n",
    "dird_df['PRJ']=dird_df['PRJ'].astype(str)\n",
    "final=dird_df.merge(file_df[['PRJ','File','SDG']], on='PRJ',how='left')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "final.head()\n",
    "final.to_excel('dird_project_summary.xlsx')"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "long_str='The idea is to run a loop from start to end and for every index in the given string check whether the sub-string can be formed from that index. This can be done by running a nested loop traversing the given string and in that loop run another loop checking for sub-string from every index. '\n",
    "short_str='for_every index'\n",
    "\n",
    "print(short_str in long_str)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('polmap': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "3c701ea15ff6b8cd345d7eda6702a5a5acbef34c5d809ba6ada83c604f672a97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}