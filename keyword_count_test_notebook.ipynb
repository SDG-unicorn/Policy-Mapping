{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Python standard library imports\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import re\n",
    "#Scientific Python ecosystem imports\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "#Text mining packages\n",
    "import nltk as nltk\n",
    "from whoosh.lang.porter import stem\n",
    "#MM Import\n",
    "import datetime as dt\n",
    "import pathlib\n",
    "import logging\n",
    "import copy\n",
    "import unicodedata\n",
    "\n",
    "from docx2python import docx2python\n",
    "from bs4 import BeautifulSoup\n",
    "import html2text\n",
    "import urllib.request\n",
    "\n",
    "from polmap.polmap import preprocess_text, doc2text # replaced the keyword processing block\n",
    "\n",
    "from collections import OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_keywords(a_string, stop_words, exception_dict=None, regex_dict=None):\n",
    "    \"\"\"\n",
    "    Prepare text for mapping.\n",
    "    \"\"\"\n",
    "    text_string = a_string\n",
    "\n",
    "    if text_string==None: #this should be moved to the prepare keywords wrapper function\n",
    "        return None\n",
    "    # if text_string is not str:\n",
    "    #     raise TypeError('text_string is not a string') \n",
    "    #     #How to return the name of the variable passed by user with format?\n",
    "    # Get error when using it with apply and lambda in pandas\n",
    "    \n",
    "    if exception_dict==None:\n",
    "        exception_dict = {\"aids\": \"ai&ds&\",\n",
    "                          \"productivity\": \"pro&ductivity&\",\n",
    "                          \"remittances\" : \"remit&tance&\"                 \n",
    "                          }\n",
    "    elif exception_dict is not dict:\n",
    "        raise TypeError('exception_dict is not of type dict')\n",
    "    \n",
    "\n",
    "    if regex_dict == None:\n",
    "        regex_dict = OrderedDict([(r'[^a-zA-Z-. ]+', ''), (r'([\\w-]+)', r' \\1 ')])\n",
    "    elif not isinstance(regex_dict, OrderedDict):\n",
    "        raise TypeError('regex_dict is not of type Ordered dict')\n",
    "        \n",
    "\n",
    "    #remove all from stop_words to keep in keywords.\n",
    "    # Review scoping rules in python, this fails with:\n",
    "    # NameError: name 'stop_words' is not defined when called in lambda function\n",
    "    # I would expect the variable to always exist whenever calling the function, but it does not.\n",
    "    #if stop_words==None:\n",
    "    #    stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "    #    stop_words.remove(\"all\")\n",
    "\n",
    "       \n",
    "    #text_string = text_string.replace('\\xa0',' ') #Remove some weird \\xa0 characters\n",
    "\n",
    "    text_string = text_string.lower().strip()\n",
    "\n",
    "    for pattern, substitution in regex_dict.items():\n",
    "        text_string = re.sub(pattern, substitution, text_string)\n",
    "    \n",
    "    #text_string = re.sub(r'([\\w-]+)', r' \\1 ', text_string) #Equivalent to center, adds leading and trailing space to the captured group\n",
    "    \n",
    "    text_string = text_string.replace(' rd ', ' R&D ')\n",
    "    \n",
    "    text_string = re.sub(r'([a-zA-z-]{3,}|ph)', r'\\1', text_string)\n",
    "    \n",
    "    # not sure this is working the way intended, \n",
    "    # if the plan was to drop two characters words,\n",
    "    # it is not  as we are however counting also spaces.\n",
    "    # an easy fix would be to move it before the centering of the terms\n",
    "    \n",
    "    for key, value in exception_dict.items(): #Protect exceptions from stemming\n",
    "        text_string = text_string.replace(key, value)\n",
    "    \n",
    "    for word in stop_words: #Remove stopwords\n",
    "        text_string = text_string.replace(' '+word+' ', '') \n",
    "    \n",
    "    text_string = re.sub(r'[a-zA-z&-]+', #Find words wirth regex. It can be improved by capturing pattern between word boundaries.\n",
    "    lambda rgx_word: ' '+stem(rgx_word.group())+' ', #Stem words, however stemming is skipped if string contains space.\n",
    "    text_string)\n",
    "        \n",
    "    for key, value in exception_dict.items(): #Restore words from exception protection\n",
    "        text_string = text_string.replace(value, key)\n",
    "    \n",
    "    text_string = ' '+text_string+' '\n",
    "    \n",
    "    text_string = re.sub(r' {2,}', r' ', text_string)\n",
    "        \n",
    "    return text_string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "######################################\n",
    "########### 2) MM Read the list of keywords and apply the prepare_keyords text processing function from polmap\n",
    "\n",
    "keys = pd.read_excel('keys_update_27012020.xlsx', sheet_name = 'Target_keys' ) #MM 'keys_from_RAKE-GBV_DB_SB_v3.xlsx', sheet_name= 'Sheet1'\n",
    "\n",
    "stop_words = set(nltk.corpus.stopwords.words(\"english\"))\n",
    "stop_words.remove(\"all\")\n",
    "\n",
    "raw_keywords_df=keys['Keys'].str.split(';', expand=True)\n",
    "\n",
    "keys['Keys']=keys['Keys'].apply(lambda keywords: re.sub(';$', '', keywords))\n",
    "keys['Keys']=keys['Keys'].apply(lambda keywords: [preprocess_text(keyword, stop_words) for keyword in keywords.split(';')])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[' multi stakehold partnership ',\n",
       " ' multi stakehold develop ',\n",
       " ' partnership sustain develop ',\n",
       " ' partnership sdg ',\n",
       " ' multi-stakehold allianc ',\n",
       " ' platform multi-stakehold dialogu sustain ',\n",
       " ' multi-stakehold dialogu platform ',\n",
       " ' multi stakehold platform ']"
      ]
     },
     "metadata": {},
     "execution_count": 14
    }
   ],
   "source": [
    "keys['Keys'][165]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_keywords_df['Keys'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(stop_words)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords = keys['Keys'].apply(pd.Series)\n",
    "\n",
    "keywords.replace(to_replace={'':None, np.NaN:None}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_keywords_df.replace(to_replace={'':None, np.NaN:None}, inplace=True)\n",
    "raw_keywords_df.drop(labels=57, axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "processed_keywords_df=raw_keywords_df.applymap(lambda x: prepare_keywords(x, stop_words))\n",
    "processed_keywords_df.replace(to_replace={'':None, np.NaN:None}, inplace=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_vs_standard=processed_keywords_df.compare(keywords, keep_shape=True)\n",
    "raw_vs_new = raw_keywords_df.compare(processed_keywords_df, keep_shape=True)\n",
    "raw_vs_standard = raw_keywords_df.compare(keywords, keep_shape=True)\n",
    "#comparison[1][1]='=IF(AND(ISBLANK(New_preprocessing!B2),ISBLANK(Standard_processing!B2)),\"\",IF(New_preprocessing!C2=Standard_preprocessing!C2,TRUE,FALSE))'\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keywords.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "processed_keywords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "raw_keywords_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with pd.ExcelWriter('updated_processed_keywords_test.xlsx', engine='xlsxwriter') as writer:\n",
    "    raw_keywords_df.to_excel(writer, sheet_name='Raw_keywords')\n",
    "    processed_keywords_df.to_excel(writer, sheet_name='New_preprocessing')\n",
    "    keywords.to_excel(writer, sheet_name='Standard_preprocessing')\n",
    "    new_vs_standard.to_excel(writer, sheet_name='New_vs_Standard')\n",
    "    raw_vs_new.to_excel(writer, sheet_name='Raw_vs_New')\n",
    "    raw_vs_standard.to_excel(writer, sheet_name='Raw_vs_Standard')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_dict = OrderedDict([(r'[^a-zA-Z- ]+', ''), (r'([\\w-]+)', r' \\1 ')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for key, value in regex_dict.items():\n",
    "    print(key, value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "regex_dict is OrderedDict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "not isinstance(regex_dict, OrderedDict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit",
   "metadata": {
    "interpreter": {
     "hash": "3c701ea15ff6b8cd345d7eda6702a5a5acbef34c5d809ba6ada83c604f672a97"
    }
   }
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}