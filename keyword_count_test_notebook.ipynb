{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "source": [
    "import re, json, pathlib, logging, time, argparse, pprint, warnings\n",
    "import datetime as dt\n",
    "import importlib.resources as rsrc\n",
    "from itertools import chain\n",
    "from operator import itemgetter \n",
    "#\n",
    "from nltk.corpus import stopwords\n",
    "import pandas as pds\n",
    "import numpy as np\n",
    "import nltk as nltk\n",
    "from nltk.corpus import stopwords\n",
    "from whoosh.lang.porter import stem\n",
    "#from googletrans import Translator\n",
    "from pygoogletranslation import Translator\n",
    "import circlify as crcf\n",
    "import matplotlib.pyplot as plt\n",
    "import random\n",
    "\n",
    "import polmap.polmap as plmp\n",
    "import polmap.clean_docs as cldc\n",
    "import postprocess.postprocess as pspr\n",
    "from docx2python import docx2python\n",
    "import docx2python.iterators as d2piter\n",
    "\n",
    "\n",
    "##MM imports\n",
    "#from polmap.polmap import make_directories, preprocess_text, doc2text, SDGrefs_mapper # replaced the keyword processing block\n",
    "\n",
    "#import fromRtoPython\n",
    "\n",
    "from docx2python import docx2python\n",
    " \n",
    "#from polmap.polmap import make_directories, preprocess_text, doc2text # replaced the keyword processing block"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "source": [
    "def jpbextract(jpb_docx):\n",
    "     \"\"\"\n",
    "     Helper function that extract and stores textual elements from a JRC Project Browser document.\n",
    "     The function relies on docx2python and the structure of JPB documents to extract the data.\n",
    "     \"\"\"\n",
    "     jpbdoc_text = docx2python(jpb_docx).text\n",
    "\n",
    "     jpbdoc_text = re.sub(r'\\n{2,}', '\\n\\n', jpbdoc_text) #Replace repeated newlines with double newlines\n",
    "     jpbdoc_text = re.sub(r'Page \\d{1,2} of\\n\\n \\d{1,2} Generated \\d{1,2}.\\d{1,2}.\\d{2,4} \\d{1,2}:\\d{1,2}:\\d{1,2}',\n",
    "                          '', jpbdoc_text) #Remove right headers\n",
    "\n",
    "     prj_id = re.search(r'PRJ Id\\n\\n(?P<prj_id>\\d{5})\\n\\nTitle', jpbdoc_text) #Capture prj_id\n",
    "     acronym = re.search(r'Acronym\\n\\n(?P<acronym>[\\w+ -]+)\\n\\nContext', jpbdoc_text) #Capture Acronymm\n",
    "     prj_id = prj_id.group('prj_id') if prj_id else \"N/A\"\n",
    "     acronym = acronym.group('acronym') if acronym else \"N/A\"\n",
    "     #Abbreviete deliverable repeats in Deliverable and Other section so that .split will occur only once for each deliverable\n",
    "     jpbdoc_text = re.sub(r'Deliverable ([A-Z]\\w+)', r'Del. \\1', jpbdoc_text) \n",
    "\n",
    "     jpbdoc_text = re.sub(f'WP 2021-{prj_id}-{acronym} - Project Sheet', '', jpbdoc_text) #Remove left headers\n",
    "\n",
    "     #Select second occurrence of  and use .start .end to find boundaries and remove it\n",
    "     ith=1\n",
    "     breaks = list(re.finditer('Methodologies used', jpbdoc_text))\n",
    "\n",
    "     if len(breaks) >= ith+1:\n",
    "          jpbdoc_text = f'{jpbdoc_text[:breaks[1].start()]}{jpbdoc_text[breaks[1].end():]}'\n",
    "     else:\n",
    "          pass\n",
    "\n",
    "     jpbdoc_text = re.sub(r'([a-z0-9]+[ ,.;:_-]*)(\\n{3,})([a-z0-9]+)', r'\\1\\3', jpbdoc_text) #Remove interruption in fields caused by headers\n",
    "\n",
    "     #Define fields of interest to be captured by index matching in JBPfile.     \n",
    "     jpb_fields = ('Project Portfolio', 'PRJ Id','Project Sheet','Commission priorities', 'Policy area','Methodologies used')\n",
    "     jpb_dict = {}  \n",
    "\n",
    "     for index, field_string in enumerate(jpbdoc_text.split('\\n\\n')):\n",
    "          if field_string in jpb_fields:\n",
    "               jpb_dict[field_string] = jpbdoc_text.split('\\n\\n')[index+1]\n",
    "     \n",
    "     #Split text at each Deliverable entry, remove the text before the Deliverable section, restore \"Deliverable\" at the beginning of each item.\n",
    "     # deliverables = [f'Deliverable\\n\\n{deliverable}' for deliverable in jpbdoc_text.split('\\n\\nDeliverable\\n\\n')[1:]]\n",
    "     #\n",
    "     # re_match={'labid': r'Deliverable\\n\\n(?P<label>\\w+)\\n\\n(?P<id>\\(\\d+\\))\\n\\nTitle',\n",
    "     #           'title': r'Title\\n\\n(?P<title>.*)\\n\\nDescription',\n",
    "     #           'description': r'Description\\n\\n(?P<description>.*)\\n\\nDel\\. Category',\n",
    "     #           'category': r'Del\\. Category\\n\\n(?P<category>.*)\\n\\nExpected release date',\n",
    "     #           'type': r'Del\\. Category\\n\\n(?P<type>.*)\\n\\nExpected release date',\n",
    "     #           'polcyc_stage': r'Stage of Policy Cycle\\n\\n(?P<polcyc_stage>.*)\\n\\n'}\n",
    "     # deliverable_fields = ('Deliverable', 'Title', 'Description', 'Del. Category', \n",
    "     #                       'Expected release date', 'Sensitive deliverable', 'Del. Type', \n",
    "     #                       'Contact', 'Del. Fun. Type', 'INS-RS', 'Note (sensitivity)', \n",
    "     #                       'Classification', 'Collaborating Unit', 'Customer DG', 'Stage of Policy Cycle')\n",
    "     # fields_of_interest = ('Deliverable', 'Title', 'Description', 'Del. Category', \n",
    "     #                       'Del. Type','Sensitive deliverable', 'Classification', 'Stage of Policy Cycle')\n",
    "     \n",
    "     # for del_idx, deliverable in enumerate(deliverables, start=1):\n",
    "     #      print(del_idx)\n",
    "     #      deliverable = re.sub(r'\\n{2,}', '\\n\\n', deliverable)\n",
    "     #      items = deliverable.split('\\n\\n')\n",
    "     #      match_dict={}\n",
    "\n",
    "     #      for item_idx, item in enumerate(items, start=1):\n",
    "     #           # try:\n",
    "     #           if item in fields_of_interest:\n",
    "     #                print(item)\n",
    "     #                match_dict[item] = items[item_idx+1] if items[item_idx+1] not in deliverable_fields else ''\n",
    "     #           # except IndexError as error:\n",
    "     #           #      if item_idx == len(items):\n",
    "     #           #           continue\n",
    "     #           #      else:\n",
    "     #           #           error\n",
    "                    \n",
    "               \n",
    "     #      # print(deliverable)\n",
    "     #      # match_dict = { key : re.match(value, deliverable).group(0) if re.match(value, deliverable) else ''\n",
    "     #      #              for key, value in re_match.items()\n",
    "     #      #              }\n",
    "     #      # del_label_id = re.match(r'Deliverable\\n\\n(?P<label>\\w+)\\n\\n(?P<id>\\(\\d+\\))\\n\\nTitle', deliverable)\n",
    "     #      # del_title = re.match(r'Title\\n\\n(?P<title>.*)\\n\\nDescription', deliverable)\n",
    "     #      # del_desc = re.match(r'Description\\n\\n(?P<description>.*)\\n\\nDel\\. Category', deliverable)\n",
    "     #      # del_cat = re.match(r'Del\\. Category\\n\\n(?P<category>.*)\\n\\nExpected release date', deliverable)\n",
    "     #      # del_type = re.match(r'Del\\. Category\\n\\n(?P<type>.*)\\n\\nExpected release date', deliverable)\n",
    "     #      # del_polcyc_stage = re.match(r'Stage of Policy Cycle\\n\\n(?P<polcyc_stage>.*)\\n\\n', deliverable)\n",
    "          \n",
    "     #      jpb_dict[f'Deliverable {del_idx}'] = match_dict  #This\n",
    "          \n",
    "     \n",
    "     if prj_id != jpb_dict['PRJ Id'] or prj_id == \"N/A\":\n",
    "          warnings.warn(\"PRJ Id from regex not captured or not matching the Prj\")\n",
    "\n",
    "     return jpb_dict, jpbdoc_text"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "jpbdoc_text = '''\"Waltz, bad nymph, for quick jigs vex.\" (28 letters)\n",
    "\"Glib jocks quiz nymph to vex dwarf.\" (28 letters)\n",
    "\"Sphinx of black quartz, judge my vow.\" (29 letters)\n",
    "\"How vexingly quick daft zebras jump!\" (30 letters)\n",
    "\"The five boxing wizards jump quickly.\" (31 letters)\n",
    "\"Jackdaws love my big sphinx of quartz.\" (31 letters)\n",
    "\"Pack my box with five dozen liquor jugs.\" (32 letters)'''\n",
    "\n",
    "ith=1\n",
    "breaks = list(re.finditer('letters', jpbdoc_text))\n",
    "\n",
    "if len(breaks) >= ith+1:\n",
    "    jpbdoc_text = f'{jpbdoc_text[:breaks[1].start()]}{jpbdoc_text[breaks[1].end():]}'\n",
    "else:\n",
    "    pass\n",
    "\n",
    "print(jpbdoc_text)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "\"Waltz, bad nymph, for quick jigs vex.\" (28 letters)\n",
      "\"Glib jocks quiz nymph to vex dwarf.\" (28 )\n",
      "\"Sphinx of black quartz, judge my vow.\" (29 letters)\n",
      "\"How vexingly quick daft zebras jump!\" (30 letters)\n",
      "\"The five boxing wizards jump quickly.\" (31 letters)\n",
      "\"Jackdaws love my big sphinx of quartz.\" (31 letters)\n",
      "\"Pack my box with five dozen liquor jugs.\" (32 letters)\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "test_dict, test_text = jpbextract(\"input/DirD/WP_2021_PP_20172_PRJ_30440_13072021.docx\")\n",
    "#deliverables = [f'Deliverable\\n\\n{deliverable}' for deliverable in test_text.split('\\n\\nDeliverable\\n\\n')[1:]]\n",
    "print(test_dict)"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "{'Project Sheet': 'WP 2021 PRJ 30440 GTCAP', 'Project Portfolio': '20172 - DIGI-CAP', 'PRJ Id': '30440', 'Methodologies used': 'GTCAP is currently the reference for various check methodologies e.g. orthoimagery, photo-interpretation, LPIS, area measurement and Sentinel-based monitoring under the checks by monitoring procedure.The uptake of new technologies involves further elaboration of these as well as an uptake/integration of cloud solutions, artificial intelligence and data interoperability.', 'Commission priorities': '1. The European Green Deal', 'Policy area': '1.8. Agriculture and food systems'}\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "jpb_object = docx2python(\"input/DirD/WP_2021_PP_20140_PRJ_30690_13072021.docx\")\n",
    "jpbdoc_list = list(d2piter.iter_at_depth(jpb_object.body, 4))\n",
    "# for index, item in enumerate(jpbdoc_list):\n",
    "#     print(item)\n",
    "#     if not (bool(jpbdoc_list[index-1]) & bool(jpbdoc_list[index+1])):\n",
    "#         if jpbdoc_list[index+2] == len(jpbdoc_list):\n",
    "#             break\n",
    "#         print(jpbdoc_list[index])\n",
    "#         del jpbdoc_list[index]\n",
    "    \n",
    "jpbdoc_text = jpb_object.text\n",
    "jpbdoc_text = re.sub(r'\\n{2,}', '\\n\\n', jpbdoc_text)\n",
    "# for item in list(d2piter.iter_at_depth(jpb_object.body, 4)):\n",
    "#     print(item)\n",
    "print(jpbdoc_text)\n",
    "##jpb_file.document\n"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [
    "dicts = []\n",
    "for file_ in files:\n",
    "    dicts.append()\n",
    "#print(dicts[-1])\n",
    "#dicts = [{\"A\": 1, \"B\": 2, \"C\": 3, \"X\": 4}, {\"A\": 5, \"B\": 6, \"C\": 7, \"Y\": 8}]\n",
    "#df1 = pd.DataFrame(columns=('Project Portfolio', 'PRJ Id','Project Sheet','Commission priorities', 'Policy area','Methodologies used'))\n",
    "df1 = pd.DataFrame(dicts)\n",
    "#.from_records(dicts, columns=('Project Portfolio', 'PRJ Id','Project Sheet','Commission priorities', 'Policy area','Methodologies used')) #df1.append(dicts, ignore_index=True, sort=False)\n",
    "\n",
    "#print(df1.head())\n",
    "\n",
    "#with open as outpath:\n",
    "df1.to_excel(f'{out_file}.xlsx')\n",
    "df1.to_json(f'{out_file}.json')\n",
    "print('Done!')  "
   ],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.9.1 64-bit ('polmap': conda)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.1"
  },
  "interpreter": {
   "hash": "3c701ea15ff6b8cd345d7eda6702a5a5acbef34c5d809ba6ada83c604f672a97"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}